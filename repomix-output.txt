This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
hooks/
  useONNXModel.ts
workers/
  types.ts
  worker_onnx_double.ts

================================================================
Files
================================================================

================
File: hooks/useONNXModel.ts
================
import { useState, useEffect, useRef, useCallback } from "react";
import type { ModelState, WorkerMessage, ModelResult } from "../workers/types";

export interface UseONNXModelOptions {
  workerPath: string;
  onError?: (error: string) => void;
}

export function useONNXModel({ workerPath, onError }: UseONNXModelOptions) {
  const [modelState, setModelState] = useState<ModelState>({
    device: null,
    loading: false,
    status: "Initializing",
    inferenceTime: null,
  });

  const [result, setResult] = useState<number[] | null>(null);

  // Worker reference
  const workerRef = useRef<Worker | null>(null);

  // Start worker
  useEffect(() => {
    if (!workerRef.current) {
      try {
        workerRef.current = new Worker(new URL(workerPath, import.meta.url), {
          type: "module",
        });
        workerRef.current.addEventListener("message", handleWorkerMessage);

        // Initialize worker, load model
        workerRef.current.postMessage({ type: "ping" });

        setModelState((prev) => ({ ...prev, loading: true }));
      } catch (error) {
        const errorMessage =
          error instanceof Error
            ? error.message
            : "Failed to initialize worker";

        // Send error
        onError?.(errorMessage);
        setModelState((prev) => ({
          ...prev,
          status: `Error: ${errorMessage}`,
          loading: false,
        }));
      }
    }

    return () => {
      if (workerRef.current) {
        workerRef.current.terminate();
        workerRef.current = null;
      }
    };
  }, [workerPath]);

  const handleWorkerMessage = useCallback(
    (event: MessageEvent<WorkerMessage>) => {
      const { type, data } = event.data;

      switch (type) {
        case "status":
          setModelState((prev) => ({ ...prev, status: data.message }));
          break;

        case "pong": {
          const { success, device, warning } = data;
          if (success) {
            setModelState((prev) => ({
              ...prev,
              loading: false,
              device,
              status: warning
                ? `Model loaded with warnings: ${warning} - You may still be able to run inference.`
                : "Model loaded successfully. Ready to run inference.",
            }));
          } else {
            setModelState((prev) => ({
              ...prev,
              loading: false,
              status: "Error loading model (check console)",
            }));
          }
          break;
        }

        case "error":
          setModelState((prev) => ({
            ...prev,
            loading: false,
            status: `Error: ${data.message}`,
          }));
          onError?.(data.message);
          break;

        case "result": {
          const result = data as ModelResult;
          setResult(result.output);
          setModelState((prev) => ({
            ...prev,
            loading: false,
            status: "Inference complete",
            inferenceTime: result.duration,
          }));
          break;
        }

        default:
          console.warn("Unknown message type:", type);
      }
    },
    [onError],
  );

  const runInference = useCallback(
    (input: number[]) => {
      if (!workerRef.current || modelState.loading) return;

      try {
        workerRef.current.postMessage({
          type: "run",
          data: { input },
        });

        setModelState((prev) => ({
          ...prev,
          loading: true,
          status: "Running inference...",
          inferenceTime: null,
        }));
        setResult(null);
      } catch (error) {
        const errorMessage =
          error instanceof Error ? error.message : "Unknown error";

        setModelState((prev) => ({
          ...prev,
          status: `Error running inference: ${errorMessage}`,
          loading: false,
        }));
        onError?.(errorMessage);
      }
    },
    [modelState.loading],
  );

  return {
    modelState,
    result,
    runInference,
  };
}

================
File: workers/types.ts
================
export interface ModelStats {
  device: string;
  loadTime: number;
}

export interface SessionResult {
  success: boolean;
  device: string;
  warning?: string;
}

export interface ModelResult {
  output: number[];
  duration: number;
}

export interface ModelState {
  device: string | null;
  loading: boolean;
  status: string;
  inferenceTime: number | null;
}

export interface WorkerMessage {
  type: "status" | "pong" | "error" | "stats" | "result";
  data: any;
}

export interface WorkerRequest {
  type: "ping" | "run" | "stats";
  data?: {
    input?: number[];
  };
}

================
File: workers/worker_onnx_double.ts
================
import * as ort from "onnxruntime-web/all";
import type {
  ModelStats,
  SessionResult,
  ModelResult,
  WorkerRequest,
  WorkerMessage,
} from "./types";
import { Tensor } from "onnxruntime-web";

// Set WASM path
ort.env.wasm.wasmPaths = "/onnxruntime-web/";

const MODEL_PATH = "/models/double_vector.onnx";

const stats: ModelStats = {
  device: "unknown",
  loadTime: 0,
};

class DoubleModel {
  private session: ort.InferenceSession | null = null;
  private buffer: ArrayBuffer | null = null;

  async loadModel(): Promise<boolean> {
    console.log("Loading model from", MODEL_PATH);
    try {
      const startTime = performance.now();

      const response = await fetch(MODEL_PATH);
      if (!response.ok) {
        throw new Error(
          `Failed to load model: ${response.status} ${response.statusText}`,
        );
      }

      this.buffer = await response.arrayBuffer();
      stats.loadTime = performance.now() - startTime;

      return true;
    } catch (error) {
      console.error("Error loading model:", error);
      throw error;
    }
  }

  async createSession(): Promise<SessionResult> {
    if (!this.buffer) {
      throw new Error("Model not loaded. Call loadModel first.");
    }

    // Try each execution provider
    for (const ep of ["webgpu", "cpu"] as const) {
      try {
        console.log(`Trying execution provider: ${ep}`);

        this.session = await ort.InferenceSession.create(this.buffer, {
          executionProviders: [ep],
        });

        stats.device = ep;
        console.log(`Successfully created session with ${ep}`);

        return { success: true, device: ep };
      } catch (e) {
        console.warn(`Execution provider ${ep} not available:`, e);
        continue;
      }
    }

    // If we get here, no execution provider worked
    throw new Error("No available execution provider");
  }

  async run(inputData: number[]): Promise<ModelResult> {
    // If session wasn't created, try one more time
    if (!this.session) {
      console.warn("Session not created before run. Attempting to create now.");
      try {
        await this.createSession();
      } catch (error) {
        console.error("Failed to create session:", error);
        throw error;
      }
    }

    try {
      const startTime = performance.now();

      // Create a tensor from the input data
      const inputTensor = new Tensor("float32", inputData, [inputData.length]);

      const results = await this.session!.run({ input: inputTensor });

      const duration = performance.now() - startTime;

      return {
        output: Array.from(results.output.data as Float32Array),
        duration,
      };
    } catch (error) {
      console.error("Error running inference:", error);
      throw error;
    }
  }
}

// Create Model instance
const model = new DoubleModel();

const sendMessage = (message: WorkerMessage) => {
  // self is the global scope in a Worker
  self.postMessage(message);
};

const sendError = (error: unknown) => {
  sendMessage({
    type: "error",
    data: {
      message: error instanceof Error ? error.message : "Unknown error",
    },
  });
};

const sendStatus = (message: string) => {
  sendMessage({
    type: "status",
    data: { message },
  });
};

const handleModelInit = async () => {
  sendStatus("Loading model...");

  try {
    await model.loadModel();
    sendStatus("Creating session...");

    const sessionResult = await model.createSession();
    sendMessage({
      type: "pong",
      data: sessionResult,
    });
  } catch (error) {
    console.error("Error during initialization:", error);
    // Still signal that we're loaded, but with a warning
    sendMessage({
      type: "pong",
      data: {
        success: true,
        device: "fallback",
        warning: error instanceof Error ? error.message : "Unknown error",
      },
    });
  }

  sendMessage({ type: "stats", data: stats });
};

const handleModelRun = async (input: unknown) => {
  if (!input) {
    throw new Error("No input provided for run command");
  }

  sendStatus("Running inference...");
  const result = await model.run(input);
  // await sleep(1000);
  sendMessage({
    type: "result",
    data: result,
  });
};

// Handle messages from the main thread
self.onmessage = async (e: MessageEvent<WorkerRequest>) => {
  const { type, data } = e.data;

  try {
    switch (type) {
      case "ping":
        await handleModelInit();
        break;

      case "run":
        await handleModelRun(data?.input);
        break;

      case "stats":
        sendMessage({ type: "stats", data: stats });
        break;

      default:
        console.error(`Unknown message type: ${type}`);
        break;
    }
  } catch (error) {
    sendError(error);
  }
};

export function sleep(ms: number) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}



================================================================
End of Codebase
================================================================
